---
title: EM Algorithm memo
author: Glidis
date: '2019-06-21'
slug: em-algorithm-memo
categories:
  - Algorithm
tags:
  - EM
---



<p>最早一次还有印象的接触EM算法已经是在财大旁听黄勉data mining课的时候。记得当时自己只是把EM当做了一种求极值的方法，在认真看了下讲义上的推导后，觉得懂了，随后就丢脑后了。</p>
<p>得亏这几年来终于养成了保存和备份学习资料的习惯，今天一时兴起，找到了当时的PPT，然后一口老血就喷了出来。 PPT上写的是MM算法，EM算法只是提了一句<code>EM algorithm for missing data(Special Case of MM)</code>。 看来是自己记错了，不过脸皮厚点来说错的还不是很远。</p>
<p>最近一次碰到EM算法则是在今年4月底的外部培训上，讲师大概举了一个例子：如何在一堆身高数据中推测出男女的比例？并在有新的身高数据进来时，推测其性别。男性女性的身高都可以认为是服从高斯分布的，这个问题就非常适合用EM算法来解决。其他的例子如不均匀硬币抛掷问题。</p>
<p>忍不住吐槽一下，培训时关于EM讲的不清不楚的，某C*DN都是复制和粘贴，某乎略好一点，但也忽略掉了一些关键的地方，还是stackoverflow良心一些。培训后的几天里，翻了一些资料后，针对高斯混合分布，自己手动用R编写一个EM算法函数的尝试在失败了多次以后也终于成功了。隔了许多天后，想起这回事，在这里权且做个备忘吧。</p>
<div id="em-" class="section level2">
<h2>EM 算法原理</h2>
<p><strong>应用场景</strong>：知道分布函数，知道有几种分布（一般是同一分布但参数不同）。</p>
<p>例如，样本由两个参数不同的高斯分布混合而成，由身高数据推断男女各自的身高分布参数（假设高斯分布的情形下），进而给定新的身高数据时可以判断其为男女的概率。</p>
<p>假定是总体<span class="math inline">\(X\)</span>是由<span class="math inline">\(M=1,2,\cdots,m\)</span>个正态分布混合得来的。第<span class="math inline">\(j \in M\)</span>个正态分布为<span class="math inline">\(N(\mu_j, \sigma^2_j)\)</span>，样本<span class="math inline">\({x_1, x_2, \cdots, x_n}\)</span>来自总体<span class="math inline">\(X\)</span>，第<span class="math inline">\(j\)</span>个正态分布贡献的比例为<span class="math inline">\(\pi_j=P(x \in N_j)\)</span>，用MLE来表示即：</p>
<p><span class="math display">\[L = \prod_if(\mu_i,\sigma_i, \pi_i; x_i) = \prod_i ( \sum_j \pi_j \phi(\mu_j, \sigma_j; x_i|j))\]</span></p>
<p>有点像profile-MLE，即两套参数需要估计（共<span class="math inline">\(n\)</span>个样本，<span class="math inline">\(3M\)</span>个待估参数）。</p>
<p>取对数，由于<span class="math inline">\(\pi_j\)</span>是概率，且对数函数是凹函数，根据Jansen不等式，有</p>
<p><span class="math display">\[\log L = \sum_i \log(\sum_j\pi_j\phi(\mu_j,\sigma_j;x_i) =\sum_i \log(E[\phi(x_i)])  \ge \sum_i E[\log(\phi(x_i))]\]</span></p>
<p>我们目地是最大化对数似然函数，得到参数矩阵<span class="math inline">\((\pi, \mu, \sigma)_{M,3}\)</span>，这对于上式的最右端，意味着只要尽量使等号成立即可。</p>
<p>但是，上式的写法并不太好，我们只简简单单用了一次Jansen不等式，对实际的求解并没有帮助。</p>
<p>可以看到，对数函数式左端log内有加权和，这样在求解必要条件一阶导数为0是比较困难的，而最右端的加权和是在log外面，可以想象到能玩花招也就在最右端项了：</p>
<p><span class="math display">\[\sum_{i=1}^n \sum_j \pi_j^{(k)} \log\left(\frac{\pi_j^{(k+1)}}{\pi_j^{(k+1)}}\phi(x_i; \mu_j^{(k)}, \sigma_j^{(k)})\right) \leq \sum_{i=1}^n\log\left(\sum_j\pi_j^{(k+1)}\frac{\pi_j^{(k)} \phi(x_i; \mu_j^{(k)}, \sigma_j^{(k)})}{\pi_j^{(k+1)}} \right) \ge \sum_{i=1}^n\sum_j\pi_j^{(k+1)}\log\left(\frac{\pi_j^{(k)} \phi(x_i; \mu_j^{(k)}, \sigma_j^{(k)})}{\pi_j^{(k+1)}} \right)\]</span></p>
<ol style="list-style-type: decimal">
<li><p>为使第二个Jansen不等式等号成立，这样就需要<span class="math inline">\(\frac{\pi_j^{(k)} \phi(x_i; \mu_j^{(k)}, \sigma_j^{(k)})}{\pi_j^{(k+1)}} = \tilde{\phi}_i\)</span>为常数(对于<span class="math inline">\(j\)</span>来说)，此时有 <span class="math display">\[\pi_{i,j}^{(k+1)} = \frac{\pi_j^{(k)} \log(\phi(x_i; \mu_j^{(k)}, \sigma_j^{(k)}))}{\sum_j\pi_j^{(k)} \log(\phi(x_i; \mu_j^{(k)}, \sigma_j^{(k)}))} \]</span> 其中<span class="math inline">\(\pi_{i,j}^{(k+1)}\)</span>是第<span class="math inline">\(i\)</span>个样本来自第<span class="math inline">\(j\)</span>个正态分布的概率。这样得到的就是一个<span class="math inline">\(n\times M\)</span>矩阵。这个矩阵接下来要用到。另外知道了每个样本属于各个分布的概率，我们容易得到新的<span class="math inline">\(\pi^{(k+1)}\)</span>，只需对上述概率矩阵每列求均值即可(原因见下文)。</p></li>
<li><p>有了概率矩阵，下一步就是更新出<span class="math inline">\((\mu^{(k+1)}, \sigma^{(k+1)})\)</span>。此时我们可以解问题 <span class="math display">\[\max_{(\mu, \sigma)} \sum_{i=1}^n \sum_j  \pi_{i,j}^{(k+1)} \log(\phi(x_i; \mu_j, \sigma_j)\]</span> 对于Gaussian混合分布，只需1阶条件容易解得： <span class="math display">\[\mu_j^{(k+1)} = \frac{\sum_i \pi_{i,j}^{(k+1)}x_i}{\sum_i \pi_{i,j}^{(k+1)}}\]</span> <span class="math display">\[\sigma_j^{(k+1)} = \sqrt \frac{\sum_i \pi_{i,j}^{(k+1)}(x_i - \mu_j^{(k+1)})^2}{\sum_i \pi_{i,j}^{(k+1)}}\]</span></p></li>
</ol>
<p>此时，有了<span class="math inline">\((\mu^{(k+1)}, \sigma^{(k+1)})\)</span>和上一步的副产品<span class="math inline">\(\pi^{(k+1)}\)</span>，迭代可以进行下去，直至收敛。</p>
<p><strong>注意1：</strong></p>
<ul>
<li>关于Jansen不等式部分，我们注意到第一个<span class="math inline">\(\leq\)</span>其实是<span class="math inline">\(&lt;\)</span>，这是因为对于第<span class="math inline">\(i\)</span>个样本，对于不同的高斯分布，其对应的likelihood是不同的。第一个<span class="math inline">\(\leq\)</span>是对所有的<span class="math inline">\(\pi_j^{(k+1)}\)</span>成立，然而第二个不等式是<span class="math inline">\(\ge\)</span>，为了保证目标函数值递增，这里只能想办法让<span class="math inline">\(\ge\)</span>变为<span class="math inline">\(=\)</span>。而且可以验证当等号成立时 <span class="math display">\[\sum_{i=1}^n\sum_j\pi_j^{(k+1)}\log(\tilde{\phi}(x_i; \mu_j^{(k)}, \sigma_j^{(k)})) = \sum_{i=1}^n\sum_j\pi_{i,j}^{(k+1)}\log(\tilde{\phi}(x_i; \mu_j^{(k)}, \sigma_j^{(k)})) = \max_{\sum_j \pi_{i,j} = 1} \sum_{i=1}^n\sum_j\pi_{i,j}^{(k+1)}\log\left(\frac{\pi_j^{(k)} \phi(x_i; \mu_j^{(k)}, \sigma_j^{(k)})}{\pi_{i,j}^{(k+1)}} \right) \]</span></li>
<li>为什么会收敛？我们先再用土法再分析一下。我们的目地是使得 <span class="math display">\[\sum_i \sum_j \log\left(\pi_j^{(k)} \phi(x_i; \mu_j^{(k)}, \sigma_j^{(k)})\right) \leq \sum_i \sum_j \log\left(\pi_j^{(k+1)} \phi(x_i; \mu_j^{(k+1)}, \sigma_j^{(k+1)})\right).\]</span> 一步一步来，
<span class="math display">\[\begin{align}
\sum_{i=1}^n \sum_j \log\left(\pi_j^{(k)} \phi(x_i; \mu_j^{(k)}, \sigma_j^{(k)})\right) &amp; = 
\sum_{i=1}^n\sum_j\pi_{i,j}^{(k+1)}\log\left(\frac{\pi_j^{(k)} \phi(x_i; \mu_j^{(k)}, \sigma_j^{(k)})}{\pi_{i,j}^{(k+1)}} \right) \\
&amp; \leq \sum_{i=1}^n\sum_j\pi_{i,j}^{(k+1)}\log\left(\frac{\pi_j^{(k)} \phi(x_i; \mu_j^{(k+1)}, \sigma_j^{(k+1)})}{\pi_{i,j}^{(k+1)}} \right) \\
&amp; = \sum_{i=1}^n \sum_j \pi_{i,j}^{(k+1)} \phi(x_i; \mu_j^{(k+1)}, \sigma_j^{(k+1)}) + 
\sum_{i=1}^n \sum_j \pi_{i,j}^{(k+1)}\log\left(\frac{\pi_j^{(k)}}{\pi_{i,j}^{(k+1)}} \right) \\
&amp; \leq \sum_{i=1}^n \sum_j \pi_{i,j}^{(k+1)} \phi(x_i; \mu_j^{(k+1)}, \sigma_j^{(k+1)}) + 
\sum_{i=1}^n \sum_j \pi_{i,j}^{(k+1)}\log\left(\frac{\pi_j^{(k+1)}}{\pi_{i,j}^{(k+1)}} \right) \\
&amp; = \sum_{i=1}^n\sum_j\pi_{i,j}^{(k+1)}\log\left(\frac{\pi_j^{(k+1)} \phi(x_i; \mu_j^{(k+1)}, \sigma_j^{(k+1)})}{\pi_{i,j}^{(k+1)}} \right) \\
&amp; \leq \sum_{i=1}^n \sum_j \log\left(\pi_j^{(k+1)} \phi(x_i; \mu_j^{(k+1)}, \sigma_j^{(k+1)})\right).
\end{align}\]</span>
第1个<span class="math inline">\(=\)</span>是使得Jansen不等式中等号成立情形，第1个<span class="math inline">\(\leq\)</span>是关于<span class="math inline">\((\mu, \sigma)\)</span>求最大值，第3个<span class="math inline">\(\leq\)</span>来自Jansen不等式，第2个<span class="math inline">\(\leq\)</span>则来自于<span class="math inline">\(\pi_j^{(k+1)} = \frac{1}{n}\sum_{i=1}^n\pi_{i,j}^{(k+1)}\)</span>是以下问题的最优解 <span class="math display">\[ \max_{\pi_1, \ldots, \pi_M}\sum_{i=1}^n \sum_{j=1}^M \pi_{i,j}^{(k+1)}\log\left(\frac{\pi_j} {\pi_{i,j}^{(k+1)}} \right) \\
  s.t. \quad \pi_j \ge 0 \quad for \quad j = 1, \ldots, M \\
  \sum_j \pi_j = 1\]</span></li>
</ul>
</div>
<div id="r" class="section level2">
<h2>R代码实现</h2>
<pre class="r"><code>## Expectation Maximize
## Gaussian Mix Model
## 

# generating data

x1 &lt;- rnorm(300, 3, 1)
x2 &lt;- rnorm(400, 0, 1)
x3 &lt;- rnorm(500, -3, 1)
x &lt;- c(x1,x2,x3)
plot(density(x), type = &#39;h&#39;)</code></pre>
<p><img src="/post/2019-06-21-em-algorithm-memo_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>## EM functions
get_theta &lt;- function(dat, k, mu_ini = runif(k, -5, 5), 
                   sig_ini = runif(k, 1, 10), pi_ini = rep(1, k)/k){
  n &lt;- length(dat)
  M_den  &lt;- mapply(function(v1,v2)dnorm(dat, v1, v2), mu_ini, sig_ini)*
                           (matrix(1,n,1)%*%matrix(pi_ini, nrow = 1))
  M_norm &lt;- t(apply(M_den, 1, function(x)x/sum(x)))
  theta_new &lt;- apply(M_norm, 2, mean)
  #ll_0 &lt;- sum(log(M_den/M_norm)*M_norm)
  ll_0 &lt;- sum(log(rowSums(M_den)))
  return(list(tht = theta_new, Pmat = M_norm, ll_0=ll_0))
}

get_musig &lt;- function(dat, Pmat){
  k &lt;- ncol(Pmat)
  dv &lt;- matrix(dat, nrow = 1)
  mu_new &lt;- dv %*% Pmat/colSums(Pmat)
  esq &lt;- (matrix(1, nrow = k, ncol = 1) %*% dv -  as.vector(mu_new))^2 
  sig_new &lt;- diag(esq %*% Pmat)/colSums(Pmat)
  return(list(mu = as.vector(mu_new), sig = sqrt(sig_new)))
}

## 迭代
k &lt;- 3
theta &lt;- get_theta(x,k)
ll_0 &lt;- theta$ll_0
i &lt;- 0
gap &lt;- 10
tol &lt;- 1e-4
while (i &lt;= 1000 &amp;&amp; gap &gt; tol) {
  new_musig &lt;- get_musig(x, theta$Pmat)
  theta &lt;- get_theta(x, k, new_musig$mu, new_musig$sig, theta$tht)
  ll_1 &lt;- theta$ll_0
  gap &lt;- ll_1 - ll_0
  ll_0 &lt;- ll_1
  i &lt;- i + 1
  if(i &gt;= 1000 || gap &lt;= tol){
    print(data.frame(theta=theta$tht, mu = new_musig$mu, sig = new_musig$sig))
  }
}</code></pre>
<pre><code>##       theta          mu       sig
## 1 0.4331997 -2.94079288 0.9691276
## 2 0.2877553  0.01692771 0.8282269
## 3 0.2790450  2.81568012 1.1450735</code></pre>
<p>也可以使用开源的R包来实现</p>
<pre class="r"><code>library(mixtools)</code></pre>
<pre><code>## mixtools package, version 1.1.0, Released 2017-03-10
## This package is based upon work supported by the National Science Foundation under Grant No. SES-0518772.</code></pre>
<pre class="r"><code>res &lt;- normalmixEM(x, c(0.2,.3, .5), runif(3, -5, 5), runif(3, 1, 10))</code></pre>
<pre><code>## number of iterations= 432</code></pre>
<pre class="r"><code>print(data.frame(theta = res$lambda, mu = res$mu, sig = res$sigma))</code></pre>
<pre><code>##       theta         mu       sig
## 1 0.4338174 -2.9385874 0.9704185
## 2 0.2804826  2.8068708 1.1497172
## 3 0.2856999  0.0143126 0.8221544</code></pre>
<p><strong>注意2：</strong> 在实际问题中，EM算法并不总是收敛的，有可能落进局部解。例如，以上例子中，我们将方差变大，结果将不在收敛。</p>
<pre class="r"><code>res &lt;- normalmixEM(c(rnorm(300, -3, 5), rnorm(400, 0, 2), rnorm(500, 3, 4)),
                   c(0.2,.3, .5), runif(3, -5, 5), runif(3, 1, 10))</code></pre>
<pre><code>## WARNING! NOT CONVERGENT! 
## number of iterations= 1000</code></pre>
<pre class="r"><code>print(data.frame(theta = res$lambda, mu = res$mu, sig = res$sigma))</code></pre>
<pre><code>##       theta         mu      sig
## 1 0.6104666  2.1342257 4.233808
## 2 0.1015034 -6.8173321 3.166085
## 3 0.2880299 -0.1653364 1.813127</code></pre>
</div>
<div id="em" class="section level2">
<h2>专业的广义的EM收敛性证明</h2>
<p>详细内容参考<strong>ESLII</strong>中<em>The EM Algorithm in General</em>节。 记<span class="math inline">\(\mathbf{Z}\)</span>为观测样本，<span class="math inline">\(\mathbf{Z}^m\)</span>为隐变量，即一系列的dummy，表示某样本是否属于某分布。另记<span class="math inline">\(\mathbf{T} = (\mathbf{Z}, \mathbf{Z}^m)\)</span>，<span class="math inline">\(\theta\)</span>为待估参数，既包含每个分布中的参数，还包含分布之间的混合比例。那么有</p>
<p><span class="math display">\[\mathbf{P}(\mathbf{Z}^m|\mathbf{Z},\theta) = \frac{\mathbf{P}(\mathbf{T}|\theta)}{\mathbf{P}(\mathbf{Z}|\theta)} \Rightarrow \mathbf{P}(\mathbf{Z}|\theta) = \frac{\mathbf{P}(\mathbf{T}|\theta)}{\mathbf{P}(\mathbf{Z}^m|\mathbf{Z},\theta)}\]</span> 转化为对数似然函数形式有 <span class="math display">\[
l(\theta^\prime;\mathbf{Z})= l_0(\theta^\prime; \mathbf{T}) - l_1(\theta^\prime; \mathbf{Z}^m|\mathbf{Z})
\]</span> 通过求条件期望（conditioning on <span class="math inline">\([\mathbf{T}|\mathbf{Z}](\theta)\)</span>）得到<span class="math inline">\(\theta^\prime\)</span>和<span class="math inline">\(\theta\)</span>之间的迭代关系， <span class="math display">\[
l(\theta^\prime;\mathbf{Z})=\mathrm{E}[ l_0(\theta^\prime; \mathbf{T})|\mathbf{Z},\theta]- \mathrm{E}[l_1(\theta^\prime; \mathbf{Z}^m|\mathbf{Z})|\mathbf{Z},\theta]
\]</span> 按本人粗浅的理解，上式是在建立迭代关系的同时，也通过积分（求期望）消除了<span class="math inline">\(\mathbf{Z}^m\)</span>。 记上式为 <span class="math display">\[l(\theta^\prime;\mathbf{Z})= Q(\theta^\prime, \theta) - R(\theta^\prime, \theta) \]</span> 由于<span class="math inline">\(\max_{\theta^\prime}l(\theta^\prime;\mathbf{Z})\)</span>难以直接求解，EM做的其实是<span class="math inline">\(\max_{\theta^\prime}Q(\theta^\prime, \theta)\)</span>，之所以能收敛，还在于另外一面<span class="math inline">\(\max_{\theta^\prime}R(\theta^\prime, \theta) = R(\theta^\prime, \theta)\)</span>。 则有 <span class="math display">\[l(\theta^\prime;\mathbf{Z})- l(\theta;\mathbf{Z}) = [Q(\theta^\prime, \theta)-Q(\theta, \theta)] + [R(\theta, \theta)-R(\theta^\prime, \theta)] \ge  0\]</span> 其实求条件期望这一操作，我也没搞懂…</p>
</div>
