---
title: helix_nn
author: glidis
date: '2018-04-30'
slug: helix-nn
categories:
  - R
  - nn
tags: []
---
生成数据
```{r}
library(ggplot2)
n <- 100
id <- 1:n
alpha1 <- pi*(id-1)/25
beta <- 1.6*(105-id)/104
x0 <- 5+beta*sin(alpha1)
y0 <- 5+beta*cos(alpha1)
z0 <- rep('yes', n)
x1 <- 5-beta*sin(alpha1)
y1 <- 5-beta*cos(alpha1)
z1 <- rep('no', n)
dat <- rbind(data.frame(x=x0,y=y0,z=z0), data.frame(x=x1,y=y1,z=z1))
```

数据例子
```{r}
dat[sample(nrow(dat), size = 10),]
```

神经网络设置，引用自<http://selbydavid.com/2018/01/09/neural-network/>。
```{r}
sigmoid <- function(x) 1 / (1 + exp(-x))

feedforward <- function(x, w1, w2) {
  z1 <- cbind(1, x) %*% w1
  h <- sigmoid(z1)
  z2 <- cbind(1, h) %*% w2
  list(output = sigmoid(z2), h = h)
}

backpropagate <- function(x, y, y_hat, w1, w2, h, learn_rate) {
  dw2 <- t(cbind(1, h)) %*% (y_hat - y)
  dh  <- (y_hat - y) %*% t(w2[-1, , drop = FALSE])
  dw1 <- t(cbind(1, x)) %*% (h * (1 - h) * dh)
  
  w1 <- w1 - learn_rate * dw1
  w2 <- w2 - learn_rate * dw2
  
  list(w1 = w1, w2 = w2)
}

train <- function(x, y, hidden = 5, learn_rate = 1e-2, iterations = 1e4) {
  d <- ncol(x) + 1
  w1 <- matrix(rnorm(d * hidden), d, hidden)
  w2 <- as.matrix(rnorm(hidden + 1))
  for (i in 1:iterations) {
    ff <- feedforward(x, w1, w2)
    bp <- backpropagate(x, y,
                        y_hat = ff$output,
                        w1, w2,
                        h = ff$h,
                        learn_rate = learn_rate)
    w1 <- bp$w1; w2 <- bp$w2
  }
  list(output = ff$output, w1 = w1, w2 = w2)
}
```

训练

```{r}
x <- data.matrix(dat[, c('x', 'y')])
z <- dat$z == 'yes'
```
```{r, eval=FALSE}
mnet5 <- train(x, z, hidden = 30, iterations = 1e5)
mean((mnet5$output > .5) == z)
```

图示

```{r}
grid <- expand.grid(x = seq(min(dat$x) - .5,
                             max(dat$x) + .5,
                             by = .05),
                    y = seq(min(dat$y) - .5,
                             max(dat$y) + .5,
                             by = .05))
```
```{r, eval=FALSE}
ff_grid <- feedforward(x = data.matrix(grid[, c('x', 'y')]),
                       w1 = mnet5$w1,
                       w2 = mnet5$w2)
## factor默认按大小顺序排后与labels一一对应，故0对应'no'
grid$z <- factor((ff_grid$output > .5) * 1,
                     labels = c("no", "yes"))

ggplot(dat) + aes(x, y, colour = z) +
  geom_point(data = grid, size = .5) +
  geom_point() +
  labs(x = 'x', y = 'y') +
  theme_bw()
```

## 跑偏
一般来说，增加隐藏节点数可以提高正确率，0.7左右的正确率比起线性分类器（甚至Logistic Regression）那可怜的正确率也算差强人意了。
不过在这种非线性且光滑函数的情形，可以想象的到一些非参数方法会有不错的效果。以GAM为例：
```{r}
library(mgcv)
dat$zz <- ifelse(dat$z == 'yes', 1, 0)
fit_gam <- gam(zz~s(x,y), data = dat)
p_gam <- predict.gam(fit_gam)
table(dat$zz, p_gam >= .5)/nrow(dat)
p_grid <- predict.gam(fit_gam, newdata = grid)
grid$z <- factor((p_grid >= 0.5)*1, labels = c('no', 'yes'))
ggplot(dat) + aes(x, y, colour = z) +
  geom_point(data = grid, size = .5) +
  geom_point() +
  labs(x = 'x', y = 'y') +
  theme_bw()
```

GAM在样本集内的预测堪称完美了(上述例子中连family都没设定，默认的gaussian，可以设置成binomial，即logistic连接函数)，可能也有人会说在边界外的预测并不怎么样，那首先就要对非参数方法在边界处的性质有个概念，在此不赘述。

## 跑偏更新20180702

研究了一下业界非常流行的评分卡模型（scorecard），在此数据集上表现如下：

```{r}
library(scorecard)
bins <- woebin(dat, y = 'zz')
dat_woe <- woebin_ply(dat, bins = bins)
fit_woe <- glm(zz ~ .-z_woe, family = binomial(), data = dat_woe)
p_woe <- predict(fit_woe, dat_woe, type = 'response')
table(dat$zz, p_woe >= .5)/nrow(dat)
grid$zz <- sample(0:1, size = nrow(grid), replace = TRUE) # useless but needed to run woebin_ply
grid_woe <- woebin_ply(grid, bins = bins)
p_grid <- predict(fit_woe, grid_woe, type = 'response')
grid$z <- factor((p_grid >= 0.5)*1, labels = c('no', 'yes'))
ggplot(dat) + aes(x, y, colour = z) +
  geom_point(data = grid, size = .5) +
  geom_point() +
  labs(x = 'x', y = 'y') +
  theme_bw()
```

## 跑偏20181215 
听同事说SVM也能达到样条函数的结果，测试如下：
```{r}
library(e1071)
fit_svm <- svm(zz~x+y, data = dat, kenel = 'polynomial',
	degree = 10)
p_svm <- predict(fit_svm)
table(dat$zz, p_svm >= .5)/nrow(dat)
p_grid <- predict(fit_svm, newdata = grid)
grid$z <- factor((p_grid >= 0.5)*1, labels = c('no', 'yes'))
ggplot(dat) + aes(x, y, colour = z) +
  geom_point(data = grid, size = .5) +
  geom_point() +
  labs(x = 'x', y = 'y') +
  theme_bw()
```

即使更换了kenel，效果也无明显改变。


## 跑偏20190120
本来一直记得要测一下XGboost和RandomForest的效果的，但每次都到手边给忘掉了

```{r}
library(xgboost)
params_list <- list(booster = 'gbtree',  eta= .03,
                    objective = 'binary:logistic',
                    eval_metric = 'rmse')
dat_xgb_train <- xgb.DMatrix(data = x, label = z) ## using 'x' and 'z' from mnet setting
fit_xgb <- xgboost(dat_xgb_train, params = params_list, nrounds = 100)
p_xgb <- predict(fit_xgb, as.matrix(dat[, 1:2]))
table(z, p_xgb >= .5)/nrow(dat)
p_grid <- predict(fit_xgb, newdata = as.matrix(grid[, 1:2]))
grid$z <- factor((p_grid >= 0.5)*1, labels = c('no', 'yes'))
ggplot(dat) + aes(x, y, colour = z) +
  geom_point(data = grid, size = .5) +
  geom_point() +
  labs(x = 'x', y = 'y') +
  theme_bw()
```

好吧，真是大杀器，贪婪算法就是能把你打哭。不过我们只是在训练集上看训练效果，从图中不难看出，overfit应该是很严重的。值得一提的是，gblinear的表现比gbtree差远了，说到底xgboost还是贪婪的树类模型，加上其他多余的设定自然会使效果大打折扣。

随机森林已经没有心情测了，我是那么的看好样条方法，好气啊！
