---
title: helix_nn
author: glidis
date: '2018-04-30'
slug: helix-nn
categories:
  - R
  - nn
tags: []
---



<p>生成数据</p>
<pre class="r"><code>library(ggplot2)
n &lt;- 100
id &lt;- 1:n
alpha1 &lt;- pi*(id-1)/25
beta &lt;- 1.6*(105-id)/104
x0 &lt;- 5+beta*sin(alpha1)
y0 &lt;- 5+beta*cos(alpha1)
z0 &lt;- rep(&#39;yes&#39;, n)
x1 &lt;- 5-beta*sin(alpha1)
y1 &lt;- 5-beta*cos(alpha1)
z1 &lt;- rep(&#39;no&#39;, n)
dat &lt;- rbind(data.frame(x=x0,y=y0,z=z0), data.frame(x=x1,y=y1,z=z1))</code></pre>
<p>数据例子</p>
<pre class="r"><code>dat[sample(nrow(dat), size = 10),]</code></pre>
<pre><code>##            x        y   z
## 5   5.741159 6.348164 yes
## 134 5.922266 5.585288  no
## 110 3.677560 4.377707  no
## 94  4.869605 5.107872 yes
## 125 4.845744 6.221064  no
## 198 5.039644 4.899870  no
## 129 5.430423 6.087123  no
## 22  5.615162 3.881024 yes
## 184 5.272783 5.173113  no
## 173 4.818769 5.457736  no</code></pre>
<p>神经网络设置，引用自<a href="http://selbydavid.com/2018/01/09/neural-network/" class="uri">http://selbydavid.com/2018/01/09/neural-network/</a>。</p>
<pre class="r"><code>sigmoid &lt;- function(x) 1 / (1 + exp(-x))

feedforward &lt;- function(x, w1, w2) {
  z1 &lt;- cbind(1, x) %*% w1
  h &lt;- sigmoid(z1)
  z2 &lt;- cbind(1, h) %*% w2
  list(output = sigmoid(z2), h = h)
}

backpropagate &lt;- function(x, y, y_hat, w1, w2, h, learn_rate) {
  dw2 &lt;- t(cbind(1, h)) %*% (y_hat - y)
  dh  &lt;- (y_hat - y) %*% t(w2[-1, , drop = FALSE])
  dw1 &lt;- t(cbind(1, x)) %*% (h * (1 - h) * dh)
  
  w1 &lt;- w1 - learn_rate * dw1
  w2 &lt;- w2 - learn_rate * dw2
  
  list(w1 = w1, w2 = w2)
}

train &lt;- function(x, y, hidden = 5, learn_rate = 1e-2, iterations = 1e4) {
  d &lt;- ncol(x) + 1
  w1 &lt;- matrix(rnorm(d * hidden), d, hidden)
  w2 &lt;- as.matrix(rnorm(hidden + 1))
  for (i in 1:iterations) {
    ff &lt;- feedforward(x, w1, w2)
    bp &lt;- backpropagate(x, y,
                        y_hat = ff$output,
                        w1, w2,
                        h = ff$h,
                        learn_rate = learn_rate)
    w1 &lt;- bp$w1; w2 &lt;- bp$w2
  }
  list(output = ff$output, w1 = w1, w2 = w2)
}</code></pre>
<p>训练</p>
<pre class="r"><code>x &lt;- data.matrix(dat[, c(&#39;x&#39;, &#39;y&#39;)])
z &lt;- dat$z == &#39;yes&#39;
mnet5 &lt;- train(x, z, hidden = 30, iterations = 1e5)
mean((mnet5$output &gt; .5) == z)</code></pre>
<p>图示</p>
<pre class="r"><code>grid &lt;- expand.grid(x = seq(min(dat$x) - .5,
                             max(dat$x) + .5,
                             by = .05),
                    y = seq(min(dat$y) - .5,
                             max(dat$y) + .5,
                             by = .05))</code></pre>
<pre class="r"><code>ff_grid &lt;- feedforward(x = data.matrix(grid[, c(&#39;x&#39;, &#39;y&#39;)]),
                       w1 = mnet5$w1,
                       w2 = mnet5$w2)
## factor默认按大小顺序排后与labels一一对应，故0对应&#39;no&#39;
grid$z &lt;- factor((ff_grid$output &gt; .5) * 1,
                     labels = c(&quot;no&quot;, &quot;yes&quot;))

ggplot(dat) + aes(x, y, colour = z) +
  geom_point(data = grid, size = .5) +
  geom_point() +
  labs(x = &#39;x&#39;, y = &#39;y&#39;) +
  theme_bw()</code></pre>
<div class="section level2">
<h2>跑偏</h2>
<p>一般来说，增加隐藏节点数可以提高正确率，0.7左右的正确率比起线性分类器（甚至Logistic Regression）那可怜的正确率也算差强人意了。 不过在这种非线性且光滑函数的情形，可以想象的到一些非参数方法会有不错的效果。以GAM为例：</p>
<pre class="r"><code>library(mgcv)</code></pre>
<pre><code>## Loading required package: nlme</code></pre>
<pre><code>## This is mgcv 1.8-24. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;.</code></pre>
<pre class="r"><code>dat$zz &lt;- ifelse(dat$z == &#39;yes&#39;, 1, 0)
fit_gam &lt;- gam(zz~s(x,y), data = dat)
p_gam &lt;- predict.gam(fit_gam)
table(dat$zz, p_gam &gt;= .5)/nrow(dat)</code></pre>
<pre><code>##    
##     FALSE TRUE
##   0  0.49 0.01
##   1  0.01 0.49</code></pre>
<pre class="r"><code>p_grid &lt;- predict.gam(fit_gam, newdata = grid)
grid$z &lt;- factor((p_grid &gt;= 0.5)*1, labels = c(&#39;no&#39;, &#39;yes&#39;))
ggplot(dat) + aes(x, y, colour = z) +
  geom_point(data = grid, size = .5) +
  geom_point() +
  labs(x = &#39;x&#39;, y = &#39;y&#39;) +
  theme_bw()</code></pre>
<p><img src="/post/2018-04-30-helix-nn_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>GAM在样本集内的预测堪称完美了(上述例子中连family都没设定，默认的gaussian，可以设置成binomial，即logistic连接函数)，可能也有人会说在边界外的预测并不怎么样，那首先就要对非参数方法在边界处的性质有个概念，在此不赘述。</p>
</div>
<div id="20180702" class="section level2">
<h2>跑偏更新20180702</h2>
<p>研究了一下业界非常流行的评分卡模型（scorecard），在此数据集上表现如下：</p>
<pre class="r"><code>library(scorecard)
bins &lt;- woebin(dat, y = &#39;zz&#39;)
dat_woe &lt;- woebin_ply(dat, bins = bins)
fit_woe &lt;- glm(zz ~ .-z_woe, family = binomial(), data = dat_woe)
p_woe &lt;- predict(fit_woe, dat_woe, type = &#39;response&#39;)
table(dat$zz, p_woe &gt;= .5)/nrow(dat)</code></pre>
<pre><code>##    
##     FALSE  TRUE
##   0 0.395 0.105
##   1 0.150 0.350</code></pre>
<pre class="r"><code>grid$zz &lt;- sample(0:1, size = nrow(grid), replace = TRUE) # useless but needed to run woebin_ply
grid_woe &lt;- woebin_ply(grid, bins = bins)
p_grid &lt;- predict(fit_woe, grid_woe, type = &#39;response&#39;)
grid$z &lt;- factor((p_grid &gt;= 0.5)*1, labels = c(&#39;no&#39;, &#39;yes&#39;))
ggplot(dat) + aes(x, y, colour = z) +
  geom_point(data = grid, size = .5) +
  geom_point() +
  labs(x = &#39;x&#39;, y = &#39;y&#39;) +
  theme_bw()</code></pre>
<p><img src="/post/2018-04-30-helix-nn_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
